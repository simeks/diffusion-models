{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Denoising Diffusion Implicit Models (DDIM)\n",
    "\n",
    "https://keras.io/examples/generative/ddim/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-20 00:48:49.331203: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-20 00:48:49.409433: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-03-20 00:48:49.783782: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-03-20 00:48:49.783846: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-03-20 00:48:49.783850: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "/home/simon/proj/diffusion-models/.env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "from tensorflow import keras\n",
    "from keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data\n",
    "dataset_name = \"oxford_flowers102\"\n",
    "dataset_repetitions = 5\n",
    "num_epochs = 1  # train for at least 50 epochs for good results\n",
    "image_size = 64\n",
    "# KID = Kernel Inception Distance, see related section\n",
    "kid_image_size = 75\n",
    "kid_diffusion_steps = 5\n",
    "plot_diffusion_steps = 20\n",
    "\n",
    "# sampling\n",
    "min_signal_rate = 0.02\n",
    "max_signal_rate = 0.95\n",
    "\n",
    "# architecture\n",
    "widths = [32, 64, 96, 128]\n",
    "block_depth = 2\n",
    "\n",
    "# optimization\n",
    "batch_size = 64\n",
    "ema = 0.999\n",
    "learning_rate = 1e-3\n",
    "weight_decay = 1e-4"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Oxford Flowers 102 dataset\n",
    "https://www.tensorflow.org/datasets/catalog/oxford_flowers102"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-20 00:48:54.088038: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-03-20 00:48:54.120704: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-03-20 00:48:54.120739: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-03-20 00:48:54.121470: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-20 00:48:54.124682: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-03-20 00:48:54.124712: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-03-20 00:48:54.124723: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-03-20 00:48:54.851815: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-03-20 00:48:54.851870: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-03-20 00:48:54.851874: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1700] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2023-03-20 00:48:54.851890: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-03-20 00:48:54.851918: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 21602 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "def preprocess_image(data):\n",
    "    # center crop image\n",
    "    height = tf.shape(data[\"image\"])[0]\n",
    "    width = tf.shape(data[\"image\"])[1]\n",
    "    crop_size = tf.minimum(height, width)\n",
    "    image = tf.image.crop_to_bounding_box(\n",
    "        data[\"image\"],\n",
    "        (height - crop_size) // 2,\n",
    "        (width - crop_size) // 2,\n",
    "        crop_size,\n",
    "        crop_size,\n",
    "    )\n",
    "\n",
    "    # resize and clip\n",
    "    # for image downsampling it is important to turn on antialiasing\n",
    "    image = tf.image.resize(image, size=[image_size, image_size], antialias=True)\n",
    "    return tf.clip_by_value(image / 255.0, 0.0, 1.0)\n",
    "\n",
    "\n",
    "def prepare_dataset(split):\n",
    "    # the validation dataset is shuffled as well, because data order matters\n",
    "    # for the KID estimation\n",
    "    return (\n",
    "        tfds.load(dataset_name, split=split, shuffle_files=True)\n",
    "        .map(preprocess_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "        .cache()\n",
    "        .repeat(dataset_repetitions)\n",
    "        .shuffle(10 * batch_size)\n",
    "        .batch(batch_size, drop_remainder=True)\n",
    "        .prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "    )\n",
    "\n",
    "\n",
    "# load dataset\n",
    "train_dataset = prepare_dataset(\"train[:80%]+validation[:80%]+test[:80%]\")\n",
    "val_dataset = prepare_dataset(\"train[80%:]+validation[80%:]+test[80%:]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ddim import DiffusionModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'image_size' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 16\u001b[0m\n\u001b[1;32m      2\u001b[0m model \u001b[39m=\u001b[39m DiffusionModel(\n\u001b[1;32m      3\u001b[0m     image_size,\n\u001b[1;32m      4\u001b[0m     widths,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     10\u001b[0m     kid_diffusion_steps\n\u001b[1;32m     11\u001b[0m )\n\u001b[1;32m     12\u001b[0m \u001b[39m# below tensorflow 2.9:\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[39m# pip install tensorflow_addons\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[39m# import tensorflow_addons as tfa\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[39m# optimizer=tfa.optimizers.AdamW\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m model\u001b[39m.\u001b[39;49mcompile(\n\u001b[1;32m     17\u001b[0m     optimizer\u001b[39m=\u001b[39;49mkeras\u001b[39m.\u001b[39;49moptimizers\u001b[39m.\u001b[39;49mexperimental\u001b[39m.\u001b[39;49mAdamW(\n\u001b[1;32m     18\u001b[0m         learning_rate\u001b[39m=\u001b[39;49mlearning_rate, weight_decay\u001b[39m=\u001b[39;49mweight_decay\n\u001b[1;32m     19\u001b[0m     ),\n\u001b[1;32m     20\u001b[0m     loss\u001b[39m=\u001b[39;49mkeras\u001b[39m.\u001b[39;49mlosses\u001b[39m.\u001b[39;49mmean_absolute_error,\n\u001b[1;32m     21\u001b[0m )\n\u001b[1;32m     22\u001b[0m \u001b[39m# pixelwise mean absolute error is used as loss\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \n\u001b[1;32m     24\u001b[0m \u001b[39m# save the best model based on the validation KID metric\u001b[39;00m\n\u001b[1;32m     25\u001b[0m checkpoint_path \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mcheckpoints/diffusion_model\u001b[39m\u001b[39m\"\u001b[39m\n",
      "File \u001b[0;32m~/proj/diffusion-models/ddim.py:43\u001b[0m, in \u001b[0;36mDiffusionModel.compile\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnoise_loss_tracker \u001b[39m=\u001b[39m keras\u001b[39m.\u001b[39mmetrics\u001b[39m.\u001b[39mMean(name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mn_loss\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     42\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mimage_loss_tracker \u001b[39m=\u001b[39m keras\u001b[39m.\u001b[39mmetrics\u001b[39m.\u001b[39mMean(name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mi_loss\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 43\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkid \u001b[39m=\u001b[39m metrics\u001b[39m.\u001b[39;49mKID(name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mkid\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[0;32m~/proj/diffusion-models/metrics.py:22\u001b[0m, in \u001b[0;36mKID.__init__\u001b[0;34m(self, name, **kwargs)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkid_tracker \u001b[39m=\u001b[39m keras\u001b[39m.\u001b[39mmetrics\u001b[39m.\u001b[39mMean(name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mkid_tracker\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     17\u001b[0m \u001b[39m# a pretrained InceptionV3 is used without its classification layer\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[39m# transform the pixel values to the 0-255 range, then use the same\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[39m# preprocessing as during pretraining\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencoder \u001b[39m=\u001b[39m keras\u001b[39m.\u001b[39mSequential(\n\u001b[1;32m     21\u001b[0m     [\n\u001b[0;32m---> 22\u001b[0m         keras\u001b[39m.\u001b[39mInput(shape\u001b[39m=\u001b[39m(image_size, image_size, \u001b[39m3\u001b[39m)),\n\u001b[1;32m     23\u001b[0m         layers\u001b[39m.\u001b[39mRescaling(\u001b[39m255.0\u001b[39m),\n\u001b[1;32m     24\u001b[0m         layers\u001b[39m.\u001b[39mResizing(height\u001b[39m=\u001b[39mkid_image_size, width\u001b[39m=\u001b[39mkid_image_size),\n\u001b[1;32m     25\u001b[0m         layers\u001b[39m.\u001b[39mLambda(keras\u001b[39m.\u001b[39mapplications\u001b[39m.\u001b[39minception_v3\u001b[39m.\u001b[39mpreprocess_input),\n\u001b[1;32m     26\u001b[0m         keras\u001b[39m.\u001b[39mapplications\u001b[39m.\u001b[39mInceptionV3(\n\u001b[1;32m     27\u001b[0m             include_top\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m     28\u001b[0m             input_shape\u001b[39m=\u001b[39m(kid_image_size, kid_image_size, \u001b[39m3\u001b[39m),\n\u001b[1;32m     29\u001b[0m             weights\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mimagenet\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     30\u001b[0m         ),\n\u001b[1;32m     31\u001b[0m         layers\u001b[39m.\u001b[39mGlobalAveragePooling2D(),\n\u001b[1;32m     32\u001b[0m     ],\n\u001b[1;32m     33\u001b[0m     name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39minception_encoder\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     34\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'image_size' is not defined"
     ]
    }
   ],
   "source": [
    "# create and compile the model\n",
    "model = DiffusionModel(\n",
    "    image_size,\n",
    "    widths,\n",
    "    block_depth,\n",
    "    batch_size,\n",
    "    ema,\n",
    "    min_signal_rate,\n",
    "    max_signal_rate,\n",
    "    kid_diffusion_steps,\n",
    "    kid_image_size\n",
    ")\n",
    "# below tensorflow 2.9:\n",
    "# pip install tensorflow_addons\n",
    "# import tensorflow_addons as tfa\n",
    "# optimizer=tfa.optimizers.AdamW\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.experimental.AdamW(\n",
    "        learning_rate=learning_rate, weight_decay=weight_decay\n",
    "    ),\n",
    "    loss=keras.losses.mean_absolute_error,\n",
    ")\n",
    "# pixelwise mean absolute error is used as loss\n",
    "\n",
    "# save the best model based on the validation KID metric\n",
    "checkpoint_path = \"checkpoints/diffusion_model\"\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_path,\n",
    "    save_weights_only=True,\n",
    "    monitor=\"val_kid\",\n",
    "    mode=\"min\",\n",
    "    save_best_only=True,\n",
    ")\n",
    "\n",
    "# calculate mean and variance of training dataset for normalization\n",
    "model.normalizer.adapt(train_dataset)\n",
    "\n",
    "# run training and plot generated images periodically\n",
    "model.fit(\n",
    "    train_dataset,\n",
    "    epochs=num_epochs,\n",
    "    validation_data=val_dataset,\n",
    "    callbacks=[\n",
    "        keras.callbacks.LambdaCallback(on_epoch_end=model.plot_images),\n",
    "        checkpoint_callback,\n",
    "    ],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)\n",
      "Cell \u001b[0;32mIn[11], line 2\u001b[0m\n",
      "\u001b[1;32m      1\u001b[0m \u001b[39m# create and compile the model\u001b[39;00m\n",
      "\u001b[0;32m----> 2\u001b[0m model \u001b[39m=\u001b[39m DiffusionModel(\n",
      "\u001b[1;32m      3\u001b[0m     image_size,\n",
      "\u001b[1;32m      4\u001b[0m     widths,\n",
      "\u001b[1;32m      5\u001b[0m     block_depth,\n",
      "\u001b[1;32m      6\u001b[0m     batch_size,\n",
      "\u001b[1;32m      7\u001b[0m     ema,\n",
      "\u001b[1;32m      8\u001b[0m     min_signal_rate,\n",
      "\u001b[1;32m      9\u001b[0m     max_signal_rate,\n",
      "\u001b[1;32m     10\u001b[0m     kid_diffusion_steps\n",
      "\u001b[1;32m     11\u001b[0m )\n",
      "\u001b[1;32m     12\u001b[0m \u001b[39m# # below tensorflow 2.9:\u001b[39;00m\n",
      "\u001b[1;32m     13\u001b[0m \u001b[39m# # pip install tensorflow_addons\u001b[39;00m\n",
      "\u001b[1;32m     14\u001b[0m \u001b[39m# # import tensorflow_addons as tfa\u001b[39;00m\n",
      "\u001b[0;32m   (...)\u001b[0m\n",
      "\u001b[1;32m     45\u001b[0m \u001b[39m#     ],\u001b[39;00m\n",
      "\u001b[1;32m     46\u001b[0m \u001b[39m# )\u001b[39;00m\n",
      "\n",
      "File \u001b[0;32m~/proj/diffusion-models/ddim.py:35\u001b[0m, in \u001b[0;36mDiffusionModel.__init__\u001b[0;34m(self, image_size, widths, block_depth, batch_size, ema, min_signal_rate, max_signal_rate, kid_diffusion_steps)\u001b[0m\n",
      "\u001b[1;32m     32\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkid_diffusion_steps \u001b[39m=\u001b[39m kid_diffusion_steps\n",
      "\u001b[1;32m     34\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnormalizer \u001b[39m=\u001b[39m layers\u001b[39m.\u001b[39mNormalization()\n",
      "\u001b[0;32m---> 35\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnetwork \u001b[39m=\u001b[39m unet\u001b[39m.\u001b[39;49mresidual_unet(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mimage_size, widths, block_depth)\n",
      "\u001b[1;32m     36\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mema_network \u001b[39m=\u001b[39m keras\u001b[39m.\u001b[39mmodels\u001b[39m.\u001b[39mclone_model(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnetwork)\n",
      "\n",
      "File \u001b[0;32m~/proj/diffusion-models/unet.py:70\u001b[0m, in \u001b[0;36mresidual_unet\u001b[0;34m(image_size, widths, block_depth)\u001b[0m\n",
      "\u001b[1;32m     67\u001b[0m noisy_images \u001b[39m=\u001b[39m keras\u001b[39m.\u001b[39mInput(shape\u001b[39m=\u001b[39m(image_size, image_size, \u001b[39m3\u001b[39m))\n",
      "\u001b[1;32m     68\u001b[0m noise_variances \u001b[39m=\u001b[39m keras\u001b[39m.\u001b[39mInput(shape\u001b[39m=\u001b[39m(\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m))\n",
      "\u001b[0;32m---> 70\u001b[0m e \u001b[39m=\u001b[39m layers\u001b[39m.\u001b[39;49mLambda(sinusoidal_embedding)(noise_variances)\n",
      "\u001b[1;32m     71\u001b[0m e \u001b[39m=\u001b[39m layers\u001b[39m.\u001b[39mUpSampling2D(size\u001b[39m=\u001b[39mimage_size, interpolation\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mnearest\u001b[39m\u001b[39m\"\u001b[39m)(e)\n",
      "\u001b[1;32m     73\u001b[0m x \u001b[39m=\u001b[39m layers\u001b[39m.\u001b[39mConv2D(widths[\u001b[39m0\u001b[39m], kernel_size\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)(noisy_images)\n",
      "\n",
      "File \u001b[0;32m~/proj/diffusion-models/.env/lib/python3.10/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "\u001b[1;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n",
      "\u001b[1;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n",
      "\u001b[0;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n",
      "\u001b[1;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n",
      "\u001b[1;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "\n",
      "File \u001b[0;32m~/proj/diffusion-models/unet.py:56\u001b[0m, in \u001b[0;36msinusoidal_embedding\u001b[0;34m(x)\u001b[0m\n",
      "\u001b[1;32m     51\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msinusoidal_embedding\u001b[39m(x):\n",
      "\u001b[1;32m     52\u001b[0m     embedding_min_frequency \u001b[39m=\u001b[39m \u001b[39m1.0\u001b[39m\n",
      "\u001b[1;32m     53\u001b[0m     frequencies \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mexp(\n",
      "\u001b[1;32m     54\u001b[0m         tf\u001b[39m.\u001b[39mlinspace(\n",
      "\u001b[1;32m     55\u001b[0m             tf\u001b[39m.\u001b[39mmath\u001b[39m.\u001b[39mlog(embedding_min_frequency),\n",
      "\u001b[0;32m---> 56\u001b[0m             tf\u001b[39m.\u001b[39mmath\u001b[39m.\u001b[39mlog(embedding_max_frequency),\n",
      "\u001b[1;32m     57\u001b[0m             embedding_dims \u001b[39m/\u001b[39m\u001b[39m/\u001b[39m \u001b[39m2\u001b[39m,\n",
      "\u001b[1;32m     58\u001b[0m         )\n",
      "\u001b[1;32m     59\u001b[0m     )\n",
      "\u001b[1;32m     60\u001b[0m     angular_speeds \u001b[39m=\u001b[39m \u001b[39m2.0\u001b[39m \u001b[39m*\u001b[39m math\u001b[39m.\u001b[39mpi \u001b[39m*\u001b[39m frequencies\n",
      "\u001b[1;32m     61\u001b[0m     embeddings \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mconcat(\n",
      "\u001b[1;32m     62\u001b[0m         [tf\u001b[39m.\u001b[39msin(angular_speeds \u001b[39m*\u001b[39m x), tf\u001b[39m.\u001b[39mcos(angular_speeds \u001b[39m*\u001b[39m x)], axis\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m\n",
      "\u001b[1;32m     63\u001b[0m     )\n",
      "\n",
      "\u001b[0;31mNameError\u001b[0m: Exception encountered when calling layer \"lambda_3\" (type Lambda).\n",
      "\n",
      "name 'embedding_max_frequency' is not defined\n",
      "\n",
      "Call arguments received by layer \"lambda_3\" (type Lambda):\n",
      "  • inputs=tf.Tensor(shape=(None, 1, 1, 1), dtype=float32)\n",
      "  • mask=None\n",
      "  • training=None\n"
     ]
    }
   ],
   "source": [
    "print(\"\\u001b[0;31m---------------------------------------------------------------------------\\u001b[0m\\n\\u001b[0;31mNameError\\u001b[0m                                 Traceback (most recent call last)\\nCell \\u001b[0;32mIn[11], line 2\\u001b[0m\\n\\u001b[1;32m      1\\u001b[0m \\u001b[39m# create and compile the model\\u001b[39;00m\\n\\u001b[0;32m----> 2\\u001b[0m model \\u001b[39m=\\u001b[39m DiffusionModel(\\n\\u001b[1;32m      3\\u001b[0m     image_size,\\n\\u001b[1;32m      4\\u001b[0m     widths,\\n\\u001b[1;32m      5\\u001b[0m     block_depth,\\n\\u001b[1;32m      6\\u001b[0m     batch_size,\\n\\u001b[1;32m      7\\u001b[0m     ema,\\n\\u001b[1;32m      8\\u001b[0m     min_signal_rate,\\n\\u001b[1;32m      9\\u001b[0m     max_signal_rate,\\n\\u001b[1;32m     10\\u001b[0m     kid_diffusion_steps\\n\\u001b[1;32m     11\\u001b[0m )\\n\\u001b[1;32m     12\\u001b[0m \\u001b[39m# # below tensorflow 2.9:\\u001b[39;00m\\n\\u001b[1;32m     13\\u001b[0m \\u001b[39m# # pip install tensorflow_addons\\u001b[39;00m\\n\\u001b[1;32m     14\\u001b[0m \\u001b[39m# # import tensorflow_addons as tfa\\u001b[39;00m\\n\\u001b[0;32m   (...)\\u001b[0m\\n\\u001b[1;32m     45\\u001b[0m \\u001b[39m#     ],\\u001b[39;00m\\n\\u001b[1;32m     46\\u001b[0m \\u001b[39m# )\\u001b[39;00m\\n\\nFile \\u001b[0;32m~/proj/diffusion-models/ddim.py:35\\u001b[0m, in \\u001b[0;36mDiffusionModel.__init__\\u001b[0;34m(self, image_size, widths, block_depth, batch_size, ema, min_signal_rate, max_signal_rate, kid_diffusion_steps)\\u001b[0m\\n\\u001b[1;32m     32\\u001b[0m \\u001b[39mself\\u001b[39m\\u001b[39m.\\u001b[39mkid_diffusion_steps \\u001b[39m=\\u001b[39m kid_diffusion_steps\\n\\u001b[1;32m     34\\u001b[0m \\u001b[39mself\\u001b[39m\\u001b[39m.\\u001b[39mnormalizer \\u001b[39m=\\u001b[39m layers\\u001b[39m.\\u001b[39mNormalization()\\n\\u001b[0;32m---> 35\\u001b[0m \\u001b[39mself\\u001b[39m\\u001b[39m.\\u001b[39mnetwork \\u001b[39m=\\u001b[39m unet\\u001b[39m.\\u001b[39;49mresidual_unet(\\u001b[39mself\\u001b[39;49m\\u001b[39m.\\u001b[39;49mimage_size, widths, block_depth)\\n\\u001b[1;32m     36\\u001b[0m \\u001b[39mself\\u001b[39m\\u001b[39m.\\u001b[39mema_network \\u001b[39m=\\u001b[39m keras\\u001b[39m.\\u001b[39mmodels\\u001b[39m.\\u001b[39mclone_model(\\u001b[39mself\\u001b[39m\\u001b[39m.\\u001b[39mnetwork)\\n\\nFile \\u001b[0;32m~/proj/diffusion-models/unet.py:70\\u001b[0m, in \\u001b[0;36mresidual_unet\\u001b[0;34m(image_size, widths, block_depth)\\u001b[0m\\n\\u001b[1;32m     67\\u001b[0m noisy_images \\u001b[39m=\\u001b[39m keras\\u001b[39m.\\u001b[39mInput(shape\\u001b[39m=\\u001b[39m(image_size, image_size, \\u001b[39m3\\u001b[39m))\\n\\u001b[1;32m     68\\u001b[0m noise_variances \\u001b[39m=\\u001b[39m keras\\u001b[39m.\\u001b[39mInput(shape\\u001b[39m=\\u001b[39m(\\u001b[39m1\\u001b[39m, \\u001b[39m1\\u001b[39m, \\u001b[39m1\\u001b[39m))\\n\\u001b[0;32m---> 70\\u001b[0m e \\u001b[39m=\\u001b[39m layers\\u001b[39m.\\u001b[39;49mLambda(sinusoidal_embedding)(noise_variances)\\n\\u001b[1;32m     71\\u001b[0m e \\u001b[39m=\\u001b[39m layers\\u001b[39m.\\u001b[39mUpSampling2D(size\\u001b[39m=\\u001b[39mimage_size, interpolation\\u001b[39m=\\u001b[39m\\u001b[39m\\\"\\u001b[39m\\u001b[39mnearest\\u001b[39m\\u001b[39m\\\"\\u001b[39m)(e)\\n\\u001b[1;32m     73\\u001b[0m x \\u001b[39m=\\u001b[39m layers\\u001b[39m.\\u001b[39mConv2D(widths[\\u001b[39m0\\u001b[39m], kernel_size\\u001b[39m=\\u001b[39m\\u001b[39m1\\u001b[39m)(noisy_images)\\n\\nFile \\u001b[0;32m~/proj/diffusion-models/.env/lib/python3.10/site-packages/keras/utils/traceback_utils.py:70\\u001b[0m, in \\u001b[0;36mfilter_traceback.<locals>.error_handler\\u001b[0;34m(*args, **kwargs)\\u001b[0m\\n\\u001b[1;32m     67\\u001b[0m     filtered_tb \\u001b[39m=\\u001b[39m _process_traceback_frames(e\\u001b[39m.\\u001b[39m__traceback__)\\n\\u001b[1;32m     68\\u001b[0m     \\u001b[39m# To get the full stack trace, call:\\u001b[39;00m\\n\\u001b[1;32m     69\\u001b[0m     \\u001b[39m# `tf.debugging.disable_traceback_filtering()`\\u001b[39;00m\\n\\u001b[0;32m---> 70\\u001b[0m     \\u001b[39mraise\\u001b[39;00m e\\u001b[39m.\\u001b[39mwith_traceback(filtered_tb) \\u001b[39mfrom\\u001b[39;00m \\u001b[39mNone\\u001b[39m\\n\\u001b[1;32m     71\\u001b[0m \\u001b[39mfinally\\u001b[39;00m:\\n\\u001b[1;32m     72\\u001b[0m     \\u001b[39mdel\\u001b[39;00m filtered_tb\\n\\nFile \\u001b[0;32m~/proj/diffusion-models/unet.py:56\\u001b[0m, in \\u001b[0;36msinusoidal_embedding\\u001b[0;34m(x)\\u001b[0m\\n\\u001b[1;32m     51\\u001b[0m \\u001b[39mdef\\u001b[39;00m \\u001b[39msinusoidal_embedding\\u001b[39m(x):\\n\\u001b[1;32m     52\\u001b[0m     embedding_min_frequency \\u001b[39m=\\u001b[39m \\u001b[39m1.0\\u001b[39m\\n\\u001b[1;32m     53\\u001b[0m     frequencies \\u001b[39m=\\u001b[39m tf\\u001b[39m.\\u001b[39mexp(\\n\\u001b[1;32m     54\\u001b[0m         tf\\u001b[39m.\\u001b[39mlinspace(\\n\\u001b[1;32m     55\\u001b[0m             tf\\u001b[39m.\\u001b[39mmath\\u001b[39m.\\u001b[39mlog(embedding_min_frequency),\\n\\u001b[0;32m---> 56\\u001b[0m             tf\\u001b[39m.\\u001b[39mmath\\u001b[39m.\\u001b[39mlog(embedding_max_frequency),\\n\\u001b[1;32m     57\\u001b[0m             embedding_dims \\u001b[39m/\\u001b[39m\\u001b[39m/\\u001b[39m \\u001b[39m2\\u001b[39m,\\n\\u001b[1;32m     58\\u001b[0m         )\\n\\u001b[1;32m     59\\u001b[0m     )\\n\\u001b[1;32m     60\\u001b[0m     angular_speeds \\u001b[39m=\\u001b[39m \\u001b[39m2.0\\u001b[39m \\u001b[39m*\\u001b[39m math\\u001b[39m.\\u001b[39mpi \\u001b[39m*\\u001b[39m frequencies\\n\\u001b[1;32m     61\\u001b[0m     embeddings \\u001b[39m=\\u001b[39m tf\\u001b[39m.\\u001b[39mconcat(\\n\\u001b[1;32m     62\\u001b[0m         [tf\\u001b[39m.\\u001b[39msin(angular_speeds \\u001b[39m*\\u001b[39m x), tf\\u001b[39m.\\u001b[39mcos(angular_speeds \\u001b[39m*\\u001b[39m x)], axis\\u001b[39m=\\u001b[39m\\u001b[39m3\\u001b[39m\\n\\u001b[1;32m     63\\u001b[0m     )\\n\\n\\u001b[0;31mNameError\\u001b[0m: Exception encountered when calling layer \\\"lambda_3\\\" (type Lambda).\\n\\nname 'embedding_max_frequency' is not defined\\n\\nCall arguments received by layer \\\"lambda_3\\\" (type Lambda):\\n  • inputs=tf.Tensor(shape=(None, 1, 1, 1), dtype=float32)\\n  • mask=None\\n  • training=None\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "efd13e6073fa95226171143455c946d52f9c84176bc88a6983ba558893b770c2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
